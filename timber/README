
random_forest
==============

RANDOM_FOREST - example of usage ensemble-based algorithms for email classification task
and feature selection.

Classification is multilabel : each document from train and test datasets should be labeled
with one of these classes:

    -- SPAM
    -- HAM
    -- INFO - newsletters and commercial emails ;
    -- NETS - emails from social networks/services .

Model was built on top of RandomForest and ExtraTrees estimators from scikit-learn Python module.

Model was designed as a wrapper over sets of classifiers and heuristic-rules to achieve following purposes:

    -- to check < rules - classifiers > combinations ;
    -- to code / test / remove new heuristic-rules ;
    -- to arrange distinct sets of heuristic-logic and apply them to different datasets.

It allows easily to plug-in Python modules with various feature selection approaches :
use SelectKBest method here to apply univariate feature selection to sparse datasets matrices.

RandomForest and ExtraTrees estimators also provide feature selection after performing data classification.
Thus, model allows to chain different feature selection methods together in different orders and to compare accuracy of classification for various combinations.

Dependencies
============

random_forest is tested to work under Python 2.7

The required dependencies are:

From nltk.corpus module we use stopwords corpora for russian, french, english languages and also SnowballStemmer corpora.

Install
=======

For resolving dependencies, please, perform the following commands :

1.  pip install -r requirements.txt

# in the root of repository, will install all necessary packages

2.
    sudo python -m nltk.downloader -d /usr/share/nltk_data stopwords
    sudo python -m nltk.downloader -d /usr/share/nltk_data

NOTICE: -d flag here specifies corpora installation location, by default it is :
            /usr/share/nltk_data in UNIX OS,

but it is also worthy to check the NLTK_DATA environment variable before installation.

Description
===========

Model was coded in such brutal way, that each type of classifiers resolves one-class
categorization problem. Results of these tests are collected for each document.

When classification finished, we iterate over labeled probabilities :
    -- if ALL probabilities values are under the threshold 30% --> we choose HAM label ;
    -- in the opposite case --> just take the label with maximum probability value.

1. random_forest.py: starts to iterate over supported classes labels,
   for each label it passes to Vectorizer class ( vectorizer.py ) the arguments list below :
   (< PATH to dir with collections >, < current label name >, < penalty score > )

2. Vectorizer instance iterates over subdirs in PATH and creates vector-form representation
   for each document. For obtaining appropriate features and calculate its values Vectorizer
   uses rules from Pattern classes. Appropriate Pattern Class is determined by label value.

3. Taking label value, Vectorizer constructs particular PatternClass. Attributes of this class
   represent features. They are calculated during initialization. So Vectorizer just calls
   PatternClass constructor.

4. Each pattern keeps its own dictionary of feature attributes. Dictionary is mutable, 
   it allows to switch on/off some features according to the current runtime conditions. 
   Patterns share rules for calculating features from Checker classes ( checkers.py ). 
   Rules are represented as attribute-methods of these classes.

5. Obtaining PatternClass instance, Vectorizer crunches its  __dict__ and performs some 
   post-processing routine : remove labels, unused attributes. It can also 
   add / remove / modify features according to runtime conditions.

6. sklearn.preprocessing.normalize - normilize values in prepared datasets
   ( use it with parameter axis=1 - independently normalize each sample )

7. Fit classifiers objects with one-class labeled dataset => for each email obtain 
   probabilities of tenancy to current processing label. Collect obtained 
   probabilities in dictionary : 
   
   { email_id : ( classifier name, class_probability % )} 
   
   for final merging.

8. Remove records with low level probabilities and print them to report as HAMs.
   For the rest of records simply take the label with maximum value.


Usage
===========

# ./random_forest.py --help
usage: random_forest [-h] [--score ] [--k-best ] [--estimators ] [--accuracy ]
                     [--criterion ] [--report ] [-v]
                     PATH

positional arguments:
  PATH            path to directory with samples

optional arguments:
  -h, --help      show this help message and exit
  --score         penalty score for matched feature, default = 1.0
  --k-best        number of best features, preselected by ANOVA F-value
                  regressors set, default = 0
  --estimators    number of trees in classifiers, default = 20
  --accuracy      path to file with verified statuses for checking accuracy
  --criterion     function to measure the quality of a split, default="gini"
  --report        path to file for dumping results
  -v              be verbose


Arguments description :

1. PATH => path to directory with required subdirs structure:

    /PATH
    ├── ham
    ├── info
    ├── nets
    ├── spam
    └── test

So we have four types of samples in appropriate catalogs and < test > subdir 
for performing cross-validation

example of usage :

# ./random_forest.py PATH

2. k-best option

   if mentioned in sys.argv list ( see example of usage below ) : X feature-vectors 
   will be pruned by SelectKBest class from sklearn.feature_selection module.

   X matrices, obtained by vectorizer module are sparse => as test function use 
   f_classif() method see description of its API here :
    
   http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif

   example of usage:

   # ./random_forest.py PATH --k-best 20

3. criterion option - the function to measure the quality of a split values into trees : 'gini', 'entropy'

   example of usage :

   # ./random_forest.py PATH --criterion gini

4. accuracy option

   if specified (value is path-string) : it parces file with verified results of 
   classification and build report with accuracy metrics

   example of usage:

   # ./random_forest.py PATH --accuracy /tmp/status

   classification report will look like this :

   Accuracy :

               precision    recall  f1-score   support

     NON SPAM       0.83      1.00      0.91        15
         SPAM       1.00      0.50      0.67         6

    avg/total       0.88      0.86      0.84        21


    PRECISION - ratio tp / (tp + fp) where tp is the number of true positives 
                and fp the number of false positives ;

       RECALL - ratio tp / (tp + fn) where tp is the number of true positives 
                and fn the number of false negatives ;

    The recall is intuitively the ability of the classifier to find all the 
    positive samples.

     F1-SCORE - can be interpreted as a weighted average of the precision and recall, 
                where an F1 score reaches its best value at 1 and worst score at 0.

    The relative contribution of precision and recall to the F1 score are equal. 
    The formula for the F1 score is:
        
       F1 = 2 * (precision * recall) / (precision + recall)

      SUPPORT - the number of occurrences of each class in y_true.

    File with verified results should have the following format :

    # cat /tmp/status
        15.eml : INFO
        25.eml : SPAM
        38.eml : HAM
        59.eml : NETS
        EOF

5. report option

   if specified : simply writes all statistics and logs with logging.INFO level in appropriate file

   example of usage:

   # ./random_forest.py PATH --report /tmp/report.log

Notice :
========

During vectorizing BeautifulSoup.UnicodeDammit object can emit into sys.stderr a lot of messages like this:

Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.

It happens because logging.filter() isn't added to logging settings and base log level for StreamHandler is INFO.
This will be fixed later.


Code ( the worst thing )
----

GIT
~~~

git clone https://github.com/ml-course-stanford/algos/tree/master/timber

